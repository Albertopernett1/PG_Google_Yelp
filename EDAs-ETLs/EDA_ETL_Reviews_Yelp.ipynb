{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Reviews EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encuentra la ubicacion de spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias para Koalas y definir alias\n",
    "import os\n",
    "from functools import reduce\n",
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuramos Spark para poder procesar de forma local archivos de gran tamaño\n",
    "conf = SparkConf().setAppName('appName').setMaster('local') \\\n",
    "    .set(\"spark.network.timeout\", \"600s\") \\\n",
    "    .set(\"spark.driver.memory\", \"12g\") \\\n",
    "    .set(\"spark.executor.memory\", \"10g\") \\\n",
    "    .set(\"spark.executor.cores\", \"4\") \\\n",
    "    .set(\"spark.dynamicAllocation.maxExecutors\", \"2\") \\\n",
    "    .set(\"spark.jars\", r\"C:\\mysql-connector-j-8.1.0\\mysql-connector-j-8.1.0.jar\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-J8U3NTT:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>appName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ebb6d64c40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found pyspark version \"3.4.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import databricks.koalas as ks\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datos y transformacion a koalas\n",
    "df_spark = spark.read.load(r'D:\\Proyecto Integrador Parquet\\Yelp\\review-002.parquet', format='parquet', inferSchema=True)\n",
    "df = df_spark.to_koalas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matías Tejerina\\AppData\\Roaming\\Python\\Python39\\site-packages\\databricks\\koalas\\internal.py:1430: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for name, col in reset_index.iteritems():\n",
      "C:\\Users\\Matías Tejerina\\AppData\\Roaming\\Python\\Python39\\site-packages\\databricks\\koalas\\internal.py:1356: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'databricks.koalas.frame.DataFrame'>\n",
      "Int64Index: 6990280 entries, 0 to 6990279\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count    Dtype  \n",
      "---  ------       --------------    -----  \n",
      " 0   review_id    6990280 non-null  object \n",
      " 1   user_id      6990280 non-null  object \n",
      " 2   business_id  6990280 non-null  object \n",
      " 3   stars        6990280 non-null  float64\n",
      " 4   useful       6990280 non-null  int64  \n",
      " 5   funny        6990280 non-null  int64  \n",
      " 6   cool         6990280 non-null  int64  \n",
      " 7   text         6990280 non-null  object \n",
      " 8   date         6990280 non-null  object \n",
      "dtypes: float64(1), int64(3), object(5)"
     ]
    }
   ],
   "source": [
    "# Informacion sobre las columnas\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Control de duplicados\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----+------+-----+----+--------------------+-------------------+\n",
      "|           review_id|             user_id|         business_id|stars|useful|funny|cool|                text|               date|\n",
      "+--------------------+--------------------+--------------------+-----+------+-----+----+--------------------+-------------------+\n",
      "|KU_O5udG6zpxOg-Vc...|mh_-eMZ6K5RLWhZyI...|XQfwVwDr-v0ZS3_Cb...|  3.0|     0|    0|   0|If you decide to ...|2018-07-07 22:09:11|\n",
      "|BiTunyQ73aT9WBnpR...|OyoGAe7OKpv6SyGZT...|7ATYjTIgM3jUlt4UM...|  5.0|     1|    0|   1|I've taken a lot ...|2012-01-03 15:28:18|\n",
      "|saUsX_uimxRlCVr67...|8g_iMtfSiwikVnbP2...|YjUWPpI6HXG530lwP...|  3.0|     0|    0|   0|Family diner. Had...|2014-02-05 20:30:30|\n",
      "|AqPFMleE6RsU23_au...|_7bHUi9Uuf5__HHc_...|kxX2SOes4o-D3ZQBk...|  5.0|     1|    0|   1|Wow!  Yummy, diff...|2015-01-04 00:01:03|\n",
      "|Sx8TMOWLNuJBWer-0...|bcjbaE6dDog4jkNY9...|e4Vwtrqf-wpJfwesg...|  4.0|     1|    0|   1|Cute interior and...|2017-01-14 20:54:15|\n",
      "|JrIxlS1TzJ-iCu79u...|eUta8W_HdHMXPzLBB...|04UD14gamNjLY0IDY...|  1.0|     1|    2|   1|I am a long term ...|2015-09-23 23:10:31|\n",
      "|6AxgBCNX_PNTOxmbR...|r3zeYsv1XFBRA4dJp...|gmjsEdUsKpj9Xxu6p...|  5.0|     0|    2|   0|Loved this tour! ...|2015-01-03 23:21:18|\n",
      "|_ZeMknuYdlQcUqng_...|yfFzsLmaWF2d4Sr0U...|LHSTtnW3YHCeUkRDG...|  5.0|     2|    0|   0|Amazingly amazing...|2015-08-07 02:29:16|\n",
      "|ZKvDG2sBvHVdF5oBN...|wSTuiTk-sKNdcFypr...|B5XSoSG3SfvQGtKEG...|  3.0|     1|    1|   0|This easter inste...|2016-03-30 22:46:33|\n",
      "|pUycOfUwM8vqX7KjR...|59MxRhNVhU9MYndMk...|gebiRewfieSdtt17P...|  3.0|     0|    0|   0|Had a party of 6 ...|2016-07-25 07:31:06|\n",
      "|rGQRf8UafX7OTlMNN...|1WHRWwQmZOZDAhp2Q...|uMvVYRgGNXf5boolA...|  5.0|     2|    0|   0|My experience wit...|2015-06-21 14:48:06|\n",
      "|l3Wk_mvAog6XANIuG...|ZbqSHbgCjzVAqaa7N...|EQ-TZ2eeD_E0BHuvo...|  4.0|     0|    0|   0|Locals recommende...|2015-08-19 14:31:45|\n",
      "|XW_LfMv0fV21l9c6x...|9OAtfnWag-ajVxRbU...|lj-E32x9_FA7GmUrB...|  4.0|     0|    0|   0|Love going here f...|2014-06-27 22:44:01|\n",
      "|8JFGBuHMoiNDyfcxu...|smOvOajNG0lS4Pq7d...|RZtGWDLCAtuipwaZ-...|  4.0|     0|    0|   0|Good food--loved ...|2009-10-14 19:57:14|\n",
      "|UBp0zWyH60Hmw6Fsa...|4Uh27DgGzsp6PqrH9...|otQS34_MymijPTdNB...|  4.0|     0|    2|   0|The bun makes the...|2011-10-27 17:12:05|\n",
      "|OAhBYw8IQ6wlfw1ow...|1C2lxzUo1Hyye4RFI...|BVndHaLihEYbr76Z0...|  5.0|     0|    0|   0|Great place for b...|2014-10-11 16:22:06|\n",
      "|oyaMhzBSwfGgemSGu...|Dd1jQj7S-BFGqRbAp...|YtSqYv1Q_pOltsVPS...|  5.0|     0|    0|   0|Tremendous servic...|2013-06-24 11:21:25|\n",
      "|LnGZB0fjfgeVDVz5I...|j2wlzrntrbKwyOcOi...|rBdG_23USc7DletfZ...|  4.0|     1|    0|   0|The hubby and I h...|2014-08-10 19:41:43|\n",
      "|u2vzZaOqJ2feRshaa...|NDZvyYHTUWWu-kqgQ...|CLEWowfkj-wKYJlQD...|  5.0|     2|    0|   1|I go to blow bar ...|2016-03-07 00:02:18|\n",
      "|Xs8Z8lmKkosqW5mw_...|IQsF3Rc6IgCzjVV9D...|eFvzHawVJofxSnD7T...|  5.0|     0|    0|   0|My absolute favor...|2014-11-12 15:30:27|\n",
      "+--------------------+--------------------+--------------------+-----+------+-----+----+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizacion del dataframe con spark\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Informacion sobre las columnas\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+------------------+------------------+-------------------+------------------+----------------------+-------------------+\n",
      "|summary|           review_id|             user_id|         business_id|             stars|            useful|              funny|              cool|                  text|               date|\n",
      "+-------+--------------------+--------------------+--------------------+------------------+------------------+-------------------+------------------+----------------------+-------------------+\n",
      "|  count|             6990280|             6990280|             6990280|           6990280|           6990280|            6990280|           6990280|               6990280|            6990280|\n",
      "|   mean|                null|                null|                null|  3.74858374771826|1.1846089140921394|0.32655959417934616|0.4986175088837643|                  null|               null|\n",
      "| stddev|                null|                null|                null|1.4787045052557126|3.2537669669333376| 1.6887290985540535|2.1724598202111562|                  null|               null|\n",
      "|    min|---4VcQZzy_vIIifU...|---1lKK3aKOuomHnw...|---kPU91CF4Lq2-Wl...|               1.0|                -1|                 -1|                -1|  !\\nMilk Bar is po...|2005-02-16 03:23:22|\n",
      "|    max|zzzz1ADBqBEVyfX4l...|zzzUFM4HFe0SFG0bP...|zzyx5x0Z7xXWWvWnZ...|               5.0|              1182|                792|               404|＼(^o^)／\\nThey hav...|2022-01-19 19:48:45|\n",
      "+-------+--------------------+--------------------+--------------------+------------------+------------------+-------------------+------------------+----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estadisticas descriptivas del dataframe con spark\n",
    "df_spark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6990280"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Longitud del dataframe\n",
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis por columna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna \"business_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150346"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cantidad de negocios con reviews\n",
    "len(df.business_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_ab50qdWOk0DdB6XOrBitw    7673\n",
       "ac1AeYqs8Z4_e2X5M3if2A    7516\n",
       "GXFMD0Z4jEVZBCsbPf4CTQ    6160\n",
       "ytynqOUb3hjKeJfRj5Tshw    5778\n",
       "oBNrLz4EDhiscSlbOl8uAw    5264\n",
       "iSRTaT9WngzB8JJ2YKJUig    5254\n",
       "VQcCL9PiNL_wkGf-uF3fjg    5146\n",
       "_C7QiQQc47AOEv4PE3Kong    4969\n",
       "GBTPC53ZrG1ZBY3DT8Mbcw    4661\n",
       "6a4gLLFSgr-Q6CZXDLzBGQ    4480\n",
       "PP3BBaVxZLcJU54uP_wL6Q    4293\n",
       "1b5mnK8bMnnju_cvU65GqQ    4247\n",
       "I_3LMZ_1m2mzR0oLIOePIg    4093\n",
       "VaO-VW3e1kARkU9bP1E7Fw    4034\n",
       "qb28j-FNX1_6xm7u372TZA    3971\n",
       "gTC8IQ_i8zXytWSly3Ttvg    3917\n",
       "yPSejq3_erxo9zdVYTBnZA    3889\n",
       "wz8ZPfySQczcPgSyd33-HQ    3634\n",
       "VVH6k9-ycttH3TV_lk5WfQ    3633\n",
       "IkY2ticzHEn4QFn8hQLSWg    3428\n",
       "Name: business_id, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 20 negocios con mas reviews\n",
    "top20_business_reviews = df.business_id.value_counts().head(20)\n",
    "top20_business_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna 'review_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6990280"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cantidad de reviews distintas\n",
    "len(df.review_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna 'user_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1987929"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cantidad de usuarios distintos\n",
    "len(df.user_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_BcWyKQL16ndpBdggh2kNA    3048\n",
       "Xw7ZjaGfr0WNVt6s_5KZfA    1840\n",
       "0Igx-a1wAstiBDerGxXk2A    1747\n",
       "-G7Zkl1wIWBBmD0KRy_sCw    1682\n",
       "ET8n-r7glWYqZhuR6GcdNw    1653\n",
       "bYENop4BuQepBjM1-BI3fA    1578\n",
       "1HM81n6n4iPIFU5d2Lokhw    1554\n",
       "fr1Hz2acAb3OaL3l6DyKNg    1447\n",
       "wXdbkFZsfDR7utJvbWElyA    1396\n",
       "Um5bfs5DH6eizgjH3xZsvg    1391\n",
       "qjfMBIZpQT9DDtw_BWCopQ    1324\n",
       "VL12EhEdT4OWqGq0nIqkzw    1308\n",
       "bJ5FtCtZX3ZZacz2_2PJjA    1298\n",
       "pou3BbKsIozfH50rxmnMew    1247\n",
       "ouODopBKF3AqfCkuQEnrDg    1129\n",
       "B-s-8VUnuBjGTP3d01jsyw    1087\n",
       "-kLVfaJytOJY2-QdQoCcNQ    1076\n",
       "vHc-UrI9yfL_pnnc6nJtyQ    1071\n",
       "CfX4sTIFFNaRchNswqhVfg    1047\n",
       "AHRrG3T1gJpHvtpZ-K0G_g    1041\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 20 usuarios que mas reviews dejaron\n",
    "df.user_id.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.51636300894046"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Promedio de reviews por usuario\n",
    "df.user_id.value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.77087790086078"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Desviacion estandar de reviews por usuario\n",
    "df.user_id.value_counts().std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna 'date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma a tipo datetime\n",
    "df.date = df.date.astype(datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\python\\pyspark\\sql\\pandas\\conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2005-02-16 03:23:22')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encuentra la review mas antigua\n",
    "df.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\python\\pyspark\\sql\\pandas\\conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2022-01-19 19:48:45')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encuentra la review mas reciente\n",
    "df.date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna 'stars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    3231627\n",
       "4.0    1452918\n",
       "1.0    1069561\n",
       "3.0     691934\n",
       "2.0     544240\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de reviews por calificacion\n",
    "df.stars.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna 'useful'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     3840492\n",
       "1     1539953\n",
       "2      687425\n",
       "3      343742\n",
       "4      186984\n",
       "5      112204\n",
       "6       71214\n",
       "7       47679\n",
       "8       34000\n",
       "9       24783\n",
       "10      18475\n",
       "11      14319\n",
       "12      11103\n",
       "13       8751\n",
       "14       7112\n",
       "15       5633\n",
       "16       4688\n",
       "17       3874\n",
       "18       3287\n",
       "19       2780\n",
       "Name: useful, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 20 numero de calificaciones 'useful' en las reviews\n",
    "df.useful.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna 'funny'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     5894117\n",
       "1      691994\n",
       "2      195290\n",
       "3       82111\n",
       "4       42254\n",
       "5       24723\n",
       "6       15545\n",
       "7       10178\n",
       "8        7147\n",
       "9        5223\n",
       "10       3739\n",
       "11       2992\n",
       "12       2367\n",
       "13       1896\n",
       "14       1419\n",
       "15       1274\n",
       "16       1014\n",
       "17        781\n",
       "18        694\n",
       "19        599\n",
       "Name: funny, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 20 numero de calificaciones funny en las reviews\n",
    "df.funny.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna 'cool'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     5377964\n",
       "1     1016736\n",
       "2      296999\n",
       "3      114763\n",
       "4       56609\n",
       "5       32352\n",
       "6       21530\n",
       "7       15010\n",
       "8       11028\n",
       "9        8085\n",
       "10       6349\n",
       "11       4981\n",
       "12       4011\n",
       "13       3125\n",
       "14       2549\n",
       "15       2105\n",
       "16       1800\n",
       "17       1458\n",
       "18       1270\n",
       "19       1092\n",
       "Name: cool, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 20 numero de calificaciones cool en las reviews\n",
    "df.cool.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6974127"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.text.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, split, size\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def count_words(text):\n",
    "    if text is not None:\n",
    "        return len(text.split())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "count_words_udf = udf(count_words, IntegerType())\n",
    "\n",
    "df_spark = df_spark.withColumn('word_count', count_words_udf(df_spark['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        word_count|\n",
      "+-------+------------------+\n",
      "|  count|           6990280|\n",
      "|   mean|104.77632326602082|\n",
      "| stddev| 97.92226559475209|\n",
      "|    min|                 1|\n",
      "|    25%|                42|\n",
      "|    50%|                75|\n",
      "|    75%|               133|\n",
      "|    max|              1070|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estadisticas descriptivas del numero de palabras por review\n",
    "df_spark.select('word_count').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Matías\n",
      "[nltk_data]     Tejerina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']\n",
    "\n",
    "udf_get_sentiment_score = udf(get_sentiment_score, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_spark.withColumn('sentiment_score', udf_get_sentiment_score('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----+------+-----+----+--------------------+-------------------+----------+---------------+\n",
      "|           review_id|             user_id|         business_id|stars|useful|funny|cool|                text|               date|word_count|sentiment_score|\n",
      "+--------------------+--------------------+--------------------+-----+------+-----+----+--------------------+-------------------+----------+---------------+\n",
      "|KU_O5udG6zpxOg-Vc...|mh_-eMZ6K5RLWhZyI...|XQfwVwDr-v0ZS3_Cb...|  3.0|     0|    0|   0|If you decide to ...|2018-07-07 22:09:11|       101|         0.8597|\n",
      "|BiTunyQ73aT9WBnpR...|OyoGAe7OKpv6SyGZT...|7ATYjTIgM3jUlt4UM...|  5.0|     1|    0|   1|I've taken a lot ...|2012-01-03 15:28:18|       151|         0.9858|\n",
      "|saUsX_uimxRlCVr67...|8g_iMtfSiwikVnbP2...|YjUWPpI6HXG530lwP...|  3.0|     0|    0|   0|Family diner. Had...|2014-02-05 20:30:30|        55|         0.9201|\n",
      "|AqPFMleE6RsU23_au...|_7bHUi9Uuf5__HHc_...|kxX2SOes4o-D3ZQBk...|  5.0|     1|    0|   1|Wow!  Yummy, diff...|2015-01-04 00:01:03|        40|         0.9588|\n",
      "|Sx8TMOWLNuJBWer-0...|bcjbaE6dDog4jkNY9...|e4Vwtrqf-wpJfwesg...|  4.0|     1|    0|   1|Cute interior and...|2017-01-14 20:54:15|        94|         0.9804|\n",
      "|JrIxlS1TzJ-iCu79u...|eUta8W_HdHMXPzLBB...|04UD14gamNjLY0IDY...|  1.0|     1|    2|   1|I am a long term ...|2015-09-23 23:10:31|        65|         0.7117|\n",
      "|6AxgBCNX_PNTOxmbR...|r3zeYsv1XFBRA4dJp...|gmjsEdUsKpj9Xxu6p...|  5.0|     0|    2|   0|Loved this tour! ...|2015-01-03 23:21:18|       152|         0.9549|\n",
      "|_ZeMknuYdlQcUqng_...|yfFzsLmaWF2d4Sr0U...|LHSTtnW3YHCeUkRDG...|  5.0|     2|    0|   0|Amazingly amazing...|2015-08-07 02:29:16|        27|         0.9706|\n",
      "|ZKvDG2sBvHVdF5oBN...|wSTuiTk-sKNdcFypr...|B5XSoSG3SfvQGtKEG...|  3.0|     1|    1|   0|This easter inste...|2016-03-30 22:46:33|       102|        -0.8995|\n",
      "|pUycOfUwM8vqX7KjR...|59MxRhNVhU9MYndMk...|gebiRewfieSdtt17P...|  3.0|     0|    0|   0|Had a party of 6 ...|2016-07-25 07:31:06|        97|         0.9782|\n",
      "|rGQRf8UafX7OTlMNN...|1WHRWwQmZOZDAhp2Q...|uMvVYRgGNXf5boolA...|  5.0|     2|    0|   0|My experience wit...|2015-06-21 14:48:06|       177|         0.9181|\n",
      "|l3Wk_mvAog6XANIuG...|ZbqSHbgCjzVAqaa7N...|EQ-TZ2eeD_E0BHuvo...|  4.0|     0|    0|   0|Locals recommende...|2015-08-19 14:31:45|        19|         0.8442|\n",
      "|XW_LfMv0fV21l9c6x...|9OAtfnWag-ajVxRbU...|lj-E32x9_FA7GmUrB...|  4.0|     0|    0|   0|Love going here f...|2014-06-27 22:44:01|        42|         0.8562|\n",
      "|8JFGBuHMoiNDyfcxu...|smOvOajNG0lS4Pq7d...|RZtGWDLCAtuipwaZ-...|  4.0|     0|    0|   0|Good food--loved ...|2009-10-14 19:57:14|        31|         0.8093|\n",
      "|UBp0zWyH60Hmw6Fsa...|4Uh27DgGzsp6PqrH9...|otQS34_MymijPTdNB...|  4.0|     0|    2|   0|The bun makes the...|2011-10-27 17:12:05|       114|         0.9768|\n",
      "|OAhBYw8IQ6wlfw1ow...|1C2lxzUo1Hyye4RFI...|BVndHaLihEYbr76Z0...|  5.0|     0|    0|   0|Great place for b...|2014-10-11 16:22:06|        29|         0.9523|\n",
      "|oyaMhzBSwfGgemSGu...|Dd1jQj7S-BFGqRbAp...|YtSqYv1Q_pOltsVPS...|  5.0|     0|    0|   0|Tremendous servic...|2013-06-24 11:21:25|        42|          0.836|\n",
      "|LnGZB0fjfgeVDVz5I...|j2wlzrntrbKwyOcOi...|rBdG_23USc7DletfZ...|  4.0|     1|    0|   0|The hubby and I h...|2014-08-10 19:41:43|       108|         0.8674|\n",
      "|u2vzZaOqJ2feRshaa...|NDZvyYHTUWWu-kqgQ...|CLEWowfkj-wKYJlQD...|  5.0|     2|    0|   1|I go to blow bar ...|2016-03-07 00:02:18|        78|          0.967|\n",
      "|Xs8Z8lmKkosqW5mw_...|IQsF3Rc6IgCzjVV9D...|eFvzHawVJofxSnD7T...|  5.0|     0|    0|   0|My absolute favor...|2014-11-12 15:30:27|        81|         0.9785|\n",
      "+--------------------+--------------------+--------------------+-----+------+-----+----+--------------------+-------------------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "df_final = df_final.withColumn('date', to_timestamp(col('date'), 'yyyy-MM-dd HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- word_count: integer (nullable = true)\n",
      " |-- sentiment_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "connection = mysql.connector.connect(host='databasegy.cdmolmugarf8.us-west-1.rds.amazonaws.com', port='3306', user='data13', password='andapashabobo13!')\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"USE database13\")\n",
    "# Crear la tabla\n",
    "tabla_sql = \"\"\"\n",
    "CREATE TABLE Reviews_Yelp (\n",
    "    review_id VARCHAR(255) PRIMARY KEY,\n",
    "    user_id VARCHAR(255),\n",
    "    business_id VARCHAR(255),\n",
    "    stars FLOAT,\n",
    "    useful INT,\n",
    "    funny INT,\n",
    "    cool INT,\n",
    "    text VARCHAR(30000),\n",
    "    date DATETIME,\n",
    "    word_count INT,\n",
    "    sentiment_score FLOAT\n",
    ")\n",
    "\"\"\"\n",
    "cursor.execute(tabla_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\mysql-connector-j-8.1.0\\\\mysql-connector-j-8.1.0.jar'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.jars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_part = df_final.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2293.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 108.0 failed 1 times, most recent failure: Lost task 11.0 in stage 108.0 (TID 804) (DESKTOP-J8U3NTT executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@71e21cd5 : C:\\Users\\Matías Tejerina\\AppData\\Local\\Temp\\blockmgr-cf4bb514-4bfa-4244-8f7e-13aafce6a98b\\22\\temp_local_037c5474-022a-42ad-adb8-2a4c56ccea68 (El sistema no puede encontrar la ruta especificada)\r\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\r\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\r\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\r\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:96)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:393)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:449)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:369)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@71e21cd5 : C:\\Users\\Matías Tejerina\\AppData\\Local\\Temp\\blockmgr-cf4bb514-4bfa-4244-8f7e-13aafce6a98b\\22\\temp_local_037c5474-022a-42ad-adb8-2a4c56ccea68 (El sistema no puede encontrar la ruta especificada)\r\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\r\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\r\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\r\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:96)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:393)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:449)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:369)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m final_df_part\u001b[39m.\u001b[39;49mwrite \\\n\u001b[0;32m      2\u001b[0m     \u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mjdbc\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      3\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mjdbc:mysql://databasegy.cdmolmugarf8.us-west-1.rds.amazonaws.com:3306/database13\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      4\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mdriver\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcom.mysql.jdbc.Driver\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      5\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mdbtable\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mReviews_Yelp\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      6\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mdata13\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      7\u001b[0m     \u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mpassword\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mandapashabobo13!\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      8\u001b[0m     \u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39mappend\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      9\u001b[0m     \u001b[39m.\u001b[39;49msave()\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\sql\\readwriter.py:1396\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mformat\u001b[39m)\n\u001b[0;32m   1395\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1396\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1398\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave(path)\n",
      "File \u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Spark\\python\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2293.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 108.0 failed 1 times, most recent failure: Lost task 11.0 in stage 108.0 (TID 804) (DESKTOP-J8U3NTT executor driver): org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@71e21cd5 : C:\\Users\\Matías Tejerina\\AppData\\Local\\Temp\\blockmgr-cf4bb514-4bfa-4244-8f7e-13aafce6a98b\\22\\temp_local_037c5474-022a-42ad-adb8-2a4c56ccea68 (El sistema no puede encontrar la ruta especificada)\r\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\r\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\r\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\r\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:96)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:393)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:449)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:369)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: error while calling spill() on org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter@71e21cd5 : C:\\Users\\Matías Tejerina\\AppData\\Local\\Temp\\blockmgr-cf4bb514-4bfa-4244-8f7e-13aafce6a98b\\22\\temp_local_037c5474-022a-42ad-adb8-2a4c56ccea68 (El sistema no puede encontrar la ruta especificada)\r\n\tat org.apache.spark.memory.TaskMemoryManager.trySpillAndAcquire(TaskMemoryManager.java:245)\r\n\tat org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:188)\r\n\tat org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:306)\r\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:96)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:393)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.allocateMemoryForRecordIfNecessary(UnsafeExternalSorter.java:449)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:487)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:369)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
     ]
    }
   ],
   "source": [
    "final_df_part.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://databasegy.cdmolmugarf8.us-west-1.rds.amazonaws.com:3306/database13\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"Reviews_Yelp\") \\\n",
    "    .option(\"user\", \"data13\") \\\n",
    "    .option(\"password\", \"andapashabobo13!\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insertar los valores en la tabla de MySQL\n",
    "#query = f\"INSERT INTO Reviews_Yelp (review_id, user_id, business_id, stars, useful, funny, cool, text, date, word_count, sentiment_score) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "#cursor.executemany(query, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cursor.close()\n",
    "#connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
